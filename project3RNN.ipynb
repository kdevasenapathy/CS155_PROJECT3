{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Shakespearean Sonnets RNN Part 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing Choices\n",
    "\n",
    "# Tokenizing your words\n",
    "# Split up sequencies\n",
    "# hypenated words\n",
    "# capitalization?\n",
    "# punctuation\n",
    "\n",
    "# What changed?\n",
    "\n",
    "# What didn't work/what changed \n",
    "\n",
    "# Analysis on the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Processing\n",
    "- I'm not actually changing anything about the given text I'm literally just tokenizing the input and then passing the training data into the LSTM model.\n",
    "- I am making 5 char jumps between samples of 40\n",
    "- I tried to use the Keras tokenizer but it wasn't recognizing all of the unique character so I wrote my own\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn text file into training set\n",
    "\n",
    "# training data should be 40 character sequences from the sonnets\n",
    "# take all possible subsequences of 40 consectives from the dataset\n",
    "# use semi-redundant requences-> pick sequences starting every n-th character\n",
    "\n",
    "\n",
    "def get_char_repr(char_to_index, char):\n",
    "    unique_words = char_to_index.keys()\n",
    "    # Return a vector that's zero everywhere besides the index corresponding to <word>\n",
    "    feature_representation = np.zeros(len(unique_words))\n",
    "    feature_representation[char_to_index[char]] = 1\n",
    "    return feature_representation \n",
    "\n",
    "def preprocess_init(text):\n",
    "    # Convert text to dataset using semi-redundant sequences\n",
    "    unique_chars = sorted(list(set(text)))\n",
    "    skip = 5 \n",
    "    char_len = 40\n",
    "    vocab_dict = {}\n",
    "    for i in range(len(unique_chars)):\n",
    "        vocab_dict[unique_chars[i]] = i\n",
    "    print(\"Dict\")\n",
    "    print(len(vocab_dict))\n",
    "    \n",
    "    #list of sets fo 40 characters\n",
    "    sequences = [] \n",
    "    # individual final characters \n",
    "    characters = []\n",
    "    \n",
    "    # generate seqs of 40 chars by looping through whole thing\n",
    "    for i in range(0, len(text)-40, skip):\n",
    "        sequences.append(text[i:i+40]) # sequence\n",
    "        characters.append(text[i+41]) # char \n",
    "    \n",
    "    # need to reshape because LSTM is being moody and wants a 3D thing for x\n",
    "    trainX = np.zeros((len(sequences), 40, len(unique_chars)))\n",
    "    trainY = np.zeros((len(sequences), len(unique_chars)))\n",
    "    # put 1s into the places where things fit the correct char\n",
    "    for index in range(len(sequences)):\n",
    "        for seq in range(len(sequences[0])):\n",
    "            trainX[index, seq] = get_char_repr(vocab_dict, sequences[index][seq])\n",
    "        trainY[index] = get_char_repr(vocab_dict, characters[index])\n",
    "        \n",
    "    return trainX,trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Dict\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "text = open(os.path.join('data/shakespeare.txt')).read()\n",
    "print(type(text))\n",
    "seqX, chars = preprocess_init(text)\n",
    "unique_chars = sorted(list(set(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19598, 71)\n",
      "(19598, 40, 71)\n"
     ]
    }
   ],
   "source": [
    "print(chars.shape)\n",
    "print(seqX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a character-based LSTM model\n",
    "\n",
    "def train_rnn(trainX, characters):\n",
    "    \n",
    "    model = Sequential()\n",
    "    # single layers of 100-200 LSTM units\n",
    "    model.add(LSTM(150, input_shape=(40,71)))\n",
    "    # fully connected dense output layer with a softmax nonlinearity\n",
    "    model.add(Dense(71, activation='softmax'))\n",
    "\n",
    "    # Train model to minimize categorical cross-entropy\n",
    "    model.compile(optimizer = 'rmsprop', # unclear if we need this\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics = [\"accuracy\"]    \n",
    "                  # we want accuracy over 0.6 on training data\n",
    "                 )\n",
    "\n",
    "    # train for many epochs so loss converges\n",
    "    print(\"Train model...\")\n",
    "    model.fit(trainX, characters, epochs = 20, batch_size = 32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model...\n",
      "Epoch 1/20\n",
      "  352/19598 [..............................] - ETA: 34s - loss: 3.6491 - accuracy: 0.1278"
     ]
    }
   ],
   "source": [
    "train_rnn(seqX, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# implement temp parameter myself as part of the poem generation algo\n",
    "# add a Lambda layer to LSTM during prediction or write a function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Poetry Generation \n",
    "\n",
    "# to generate poems, draw softmax samples from the trained model\n",
    "# Mess with temperature parameter? to control variance of your sampled text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
