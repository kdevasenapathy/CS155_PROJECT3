{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Shakespearean Sonnets RNN Part 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing Choices\n",
    "\n",
    "# Tokenizing your words\n",
    "# Split up sequencies\n",
    "# hypenated words\n",
    "# capitalization?\n",
    "# punctuation\n",
    "\n",
    "# What changed?\n",
    "\n",
    "# What didn't work/what changed \n",
    "\n",
    "# Analysis on the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import fileinput\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Processing\n",
    "- I'm not actually changing anything about the given text I'm literally just tokenizing the input and then passing the training data into the LSTM model.\n",
    "- I am making 5 char jumps between samples of 40\n",
    "- I tried to use the Keras tokenizer but it wasn't recognizing all of the unique character so I wrote my own\n",
    "- I also decided to remove new line characters because it ruined the format of the sonnet \n",
    "- I had a very high number of spaces, so I also removed white space from between the sonnets in the given text file and the numbers of the sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn text file into training set\n",
    "\n",
    "# training data should be 40 character sequences from the sonnets\n",
    "# take all possible subsequences of 40 consectives from the dataset\n",
    "# use semi-redundant requences-> pick sequences starting every n-th character\n",
    "\n",
    "\n",
    "def get_char_repr(char_to_index, char):\n",
    "    unique_words = char_to_index.keys()\n",
    "    # Return a vector that's zero everywhere besides the index corresponding to <word>\n",
    "    feature_representation = np.zeros(len(unique_words))\n",
    "    feature_representation[char_to_index[char]] = 1\n",
    "    return feature_representation \n",
    "\n",
    "def preprocess_init(text):\n",
    "    # Convert text to dataset using semi-redundant sequences\n",
    "    unique_chars = sorted(list(set(text)))\n",
    "    skip = 5 \n",
    "    char_len = 40\n",
    "    vocab_dict = {}\n",
    "    for i in range(len(unique_chars)):\n",
    "        vocab_dict[unique_chars[i]] = i\n",
    "    \n",
    "    #list of sets fo 40 characters\n",
    "    sequences = [] \n",
    "    # individual final characters \n",
    "    characters = []\n",
    "    \n",
    "    # generate seqs of 40 chars by looping through whole thing\n",
    "    for i in range(0, len(text)-40, skip):\n",
    "        sequences.append(text[i:i+40]) # sequence\n",
    "        characters.append(text[i+41]) # char \n",
    "    \n",
    "    # need to reshape because LSTM is being moody and wants a 3D thing for x\n",
    "    trainX = np.zeros((len(sequences), 40, len(unique_chars)))\n",
    "    trainY = np.zeros((len(sequences), len(unique_chars)))\n",
    "    # put 1s into the places where things fit the correct char\n",
    "    for index in range(len(sequences)):\n",
    "        for seq in range(len(sequences[0])):\n",
    "            trainX[index, seq] = get_char_repr(vocab_dict, sequences[index][seq])\n",
    "        trainY[index] = get_char_repr(vocab_dict, characters[index])\n",
    "        \n",
    "    return trainX,trainY, vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for line in fileinput.FileInput('data/shakespeare.txt'):\n",
    "    if line.rstrip():\n",
    "        text+=line.lstrip()\n",
    "text = text.replace(\"\\n\",\"\")\n",
    "text = text.replace(\"1\",\"\")\n",
    "text = text.replace(\"2\",\"\")\n",
    "text = text.replace(\"3\",\"\")\n",
    "text = text.replace(\"4\",\"\")\n",
    "text = text.replace(\"5\",\"\")\n",
    "text = text.replace(\"6\",\"\")\n",
    "text = text.replace(\"7\",\"\")\n",
    "text = text.replace(\"8\",\"\")\n",
    "text = text.replace(\"9\",\"\")\n",
    "text = text.lower()\n",
    "seqX, chars, vocab_map = preprocess_init(text)\n",
    "unique_chars = sorted(list(set(text)))\n",
    "map_len = len(vocab_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18301, 38)\n",
      "(18301, 40, 38)\n"
     ]
    }
   ],
   "source": [
    "print(chars.shape)\n",
    "print(seqX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a character-based LSTM model\n",
    "\n",
    "def train_rnn(trainX, characters):\n",
    "    \n",
    "    model = Sequential()\n",
    "    # single layers of 100-200 LSTM units\n",
    "    model.add(LSTM(200, input_shape=(40,map_len)))\n",
    "    # fully connected dense output layer with a softmax nonlinearity\n",
    "    model.add(Dense(map_len, activation='softmax'))\n",
    "\n",
    "    # Train model to minimize categorical cross-entropy\n",
    "    model.compile(optimizer = 'rmsprop', # unclear if we need this\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics = [\"accuracy\"]    \n",
    "                  # we want accuracy over 0.6 on training data\n",
    "                 )\n",
    "\n",
    "    # train for many epochs so loss converges\n",
    "    print(\"Train model...\")\n",
    "    model.fit(trainX, characters, epochs = 30, batch_size = 32)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model...\n",
      "Epoch 1/30\n",
      "18301/18301 [==============================] - 31s 2ms/step - loss: 2.9021 - accuracy: 0.1847\n",
      "Epoch 2/30\n",
      "18301/18301 [==============================] - 34s 2ms/step - loss: 2.7349 - accuracy: 0.2181\n",
      "Epoch 3/30\n",
      "18301/18301 [==============================] - 29s 2ms/step - loss: 2.6564 - accuracy: 0.2383\n",
      "Epoch 4/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 2.6034 - accuracy: 0.2498\n",
      "Epoch 5/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 2.5568 - accuracy: 0.2622\n",
      "Epoch 6/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 2.5113 - accuracy: 0.2740\n",
      "Epoch 7/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 2.4625 - accuracy: 0.2833\n",
      "Epoch 8/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 2.4068 - accuracy: 0.2958\n",
      "Epoch 9/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 2.3434 - accuracy: 0.3106\n",
      "Epoch 10/30\n",
      "18301/18301 [==============================] - 29s 2ms/step - loss: 2.2716 - accuracy: 0.3299\n",
      "Epoch 11/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 2.1859 - accuracy: 0.3553\n",
      "Epoch 12/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 2.0928 - accuracy: 0.3790\n",
      "Epoch 13/30\n",
      "18301/18301 [==============================] - 29s 2ms/step - loss: 1.9920 - accuracy: 0.4095\n",
      "Epoch 14/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 1.8878 - accuracy: 0.4404\n",
      "Epoch 15/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 1.7748 - accuracy: 0.4705\n",
      "Epoch 16/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 1.6587 - accuracy: 0.5063\n",
      "Epoch 17/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 1.5439 - accuracy: 0.5389\n",
      "Epoch 18/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 1.4319 - accuracy: 0.5799\n",
      "Epoch 19/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 1.3196 - accuracy: 0.6105\n",
      "Epoch 20/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 1.2176 - accuracy: 0.6442\n",
      "Epoch 21/30\n",
      "18301/18301 [==============================] - 29s 2ms/step - loss: 1.1200 - accuracy: 0.6751\n",
      "Epoch 22/30\n",
      "18301/18301 [==============================] - 29s 2ms/step - loss: 1.0269 - accuracy: 0.7054\n",
      "Epoch 23/30\n",
      "18301/18301 [==============================] - 29s 2ms/step - loss: 0.9434 - accuracy: 0.7296\n",
      "Epoch 24/30\n",
      "18301/18301 [==============================] - 32s 2ms/step - loss: 0.8656 - accuracy: 0.7535\n",
      "Epoch 25/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 0.8001 - accuracy: 0.7777\n",
      "Epoch 26/30\n",
      "18301/18301 [==============================] - 29s 2ms/step - loss: 0.7374 - accuracy: 0.7960\n",
      "Epoch 27/30\n",
      "18301/18301 [==============================] - 28s 2ms/step - loss: 0.6841 - accuracy: 0.8130\n",
      "Epoch 28/30\n",
      "18301/18301 [==============================] - 29s 2ms/step - loss: 0.6370 - accuracy: 0.8261\n",
      "Epoch 29/30\n",
      "18301/18301 [==============================] - 29s 2ms/step - loss: 0.5898 - accuracy: 0.8438\n",
      "Epoch 30/30\n",
      "18301/18301 [==============================] - 29s 2ms/step - loss: 0.5489 - accuracy: 0.8546\n"
     ]
    }
   ],
   "source": [
    "rnn_model = train_rnn(seqX, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_char(z_vector,temperature):\n",
    "    # implement temp parameter myself as part of the poem generation algo\n",
    "    # add a Lambda layer to LSTM during prediction or write a function\n",
    "    z_vector = np.asarray(z_vector)\n",
    "    num = np.exp(z_vector/temperature)\n",
    "    denom = np.sum(np.exp(z_vector/temperature))\n",
    "    index_predictions = num/denom\n",
    "    max_prediction_index = np.argmax(index_predictions)\n",
    "    \n",
    "    # have to find the char that goes with the index\n",
    "    char = \"\"\n",
    "    for i in vocab_map.keys():\n",
    "        if vocab_map[i] == max_prediction_index:\n",
    "            char = i\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Poetry Generation \n",
    "# to generate poems, draw softmax samples from the trained model\n",
    "def generate_sonnet_without_random_seed():\n",
    "    generated_sonnet = ''\n",
    "    seed = \"shall i compare thee to a summer's day? \" # first sentence \n",
    "    generated_sonnet += seed\n",
    "    for line in range(13*40): # of there are 14 lines in a sonnet and our seed is the first\n",
    "        # each line is about 40 characters long?\n",
    "            \n",
    "        features = np.zeros((1,40,map_len))\n",
    "        for i in range(40):\n",
    "            features[0,i] = get_char_repr(vocab_map, generated_sonnet[len(generated_sonnet)-40+i])\n",
    "        \n",
    "        # generate the next value \n",
    "        predictions = rnn_model.predict(features)[0]\n",
    "        next_char = generate_next_char(predictions, 0.25) # change around the temp\n",
    "        generated_sonnet += next_char \n",
    "        \n",
    "    # separate poem by line and print out\n",
    "    for i in range(13):\n",
    "        print(generated_sonnet[i*40:(i+1)*40])\n",
    "    return generated_sonnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day? \n",
      "hvrmaee o tysepttsettlgfrgtwiciu o eoe f\n",
      "et rnee  ns lvee o e erae.had hah hseni \n",
      "aa ty uddan ee ee hrietonmsapigta,nt frs\n",
      " tm evrue o a adyte ee hrmgindo o o o en\n",
      "etrngtiltwigigat haefrngi o o onestaditi\n",
      "t,ta oneo fnttrmgis i,o,onoh hswlss ifon\n",
      "wlswa asatntnd nttad o hnettnssttt'gtfn \n",
      "wmne hae wih htiodih hswisot hah har hah\n",
      " hrwihihdd nweo ntgt,lfgtogtmnyubtft lvs\n",
      "rcoie o al o evre.hnrswas hshvr i o adia\n",
      "tnsdsem hreeint,o ad htetntadehb ngnud o\n",
      " hvit o one ha  iln e eeprettogt,hwrn ha\n"
     ]
    }
   ],
   "source": [
    "g = generate_sonnet_without_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sonnet_random_seed():\n",
    "    generated_sonnet = ''\n",
    "    start = random.randint(1,len(text)-40)\n",
    "    seed = text[start:start+40] # first sentence \n",
    "    generated_sonnet += seed\n",
    "    for line in range(13*40): # of there are 14 lines in a sonnet and our seed is the first\n",
    "        # each line is about 40 characters long?\n",
    "            \n",
    "        features = np.zeros((1,40,map_len))\n",
    "        for i in range(40):\n",
    "            features[0,i] = get_char_repr(vocab_map, generated_sonnet[len(generated_sonnet)-40+i])\n",
    "        \n",
    "        # generate the next value \n",
    "        predictions = rnn_model.predict(features)[0]\n",
    "        next_char = generate_next_char(predictions, 0.25) # change around the temp\n",
    "        generated_sonnet += next_char \n",
    "        \n",
    "    # separate poem by line and print out\n",
    "    for i in range(13):\n",
    "        print(generated_sonnet[i*40:(i+1)*40])\n",
    "    return generated_sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tue only is their show,they live unwooed\n",
      "w h eieton tat frm eeie o a i esstl''rtt\n",
      "tageofac u u nngt  eeo o i i ene nwenlig\n",
      "tigigeti ah oe osies,nw hrwes ha  ons i \n",
      "cilsan i o e etnyudfngtih hne o esee,ha \n",
      "ihsat t evde ottnwwtat hvretesin onha o \n",
      "hsieto otonwswtfrsoo hngttt ettie i o ad\n",
      "etmnm i esiettrtatalie  fott,o o tys tyt\n",
      " efrmte i n e ecsntrn hag,hwen hae on og\n",
      "e,ad frn hvrewrno otan oeenrsteigtad tae\n",
      "eo fnr raa e hvr  isntrcloe o i a etan e\n",
      "edb,hm hrds hah wensntogttig ta ifersstf\n",
      "ltttig fa eoe hvi i,hni hsepis afil,tyso\n"
     ]
    }
   ],
   "source": [
    "g1 = generate_sonnet_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
