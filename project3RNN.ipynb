{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Shakespearean Sonnets RNN Part 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import fileinput\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Processing\n",
    "- I'm not actually changing anything about the given text I'm literally just tokenizing the input and then passing the training data into the LSTM model.\n",
    "- I am making 5 char jumps between samples of 40\n",
    "- I tried to use the Keras tokenizer but it wasn't recognizing all of the unique character so I wrote my own\n",
    "- I also decided to remove new line characters because it ruined the format of the sonnet \n",
    "- I had a very high number of spaces, so I also removed white space from between the sonnets in the given text file and the numbers of the sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn text file into training set\n",
    "\n",
    "# training data should be 40 character sequences from the sonnets\n",
    "# take all possible subsequences of 40 consectives from the dataset\n",
    "# use semi-redundant requences-> pick sequences starting every n-th character\n",
    "\n",
    "\n",
    "def get_char_repr(char_to_index, char):\n",
    "    unique_words = char_to_index.keys()\n",
    "    # Return a vector that's zero everywhere besides the index corresponding to <word>\n",
    "    feature_representation = np.zeros(len(unique_words))\n",
    "    feature_representation[char_to_index[char]] = 1\n",
    "    return feature_representation \n",
    "\n",
    "def preprocess_init(text):\n",
    "    # Convert text to dataset using semi-redundant sequences\n",
    "    text = text.replace(\"\\n\",\"\") # maybe add this back? idk \n",
    "    text = text.replace(\"0\",\"\")\n",
    "    text = text.replace(\"1\",\"\")\n",
    "    text = text.replace(\"2\",\"\")\n",
    "    text = text.replace(\"3\",\"\")\n",
    "    text = text.replace(\"4\",\"\")\n",
    "    text = text.replace(\"5\",\"\")\n",
    "    text = text.replace(\"6\",\"\")\n",
    "    text = text.replace(\"7\",\"\")\n",
    "    text = text.replace(\"8\",\"\")\n",
    "    text = text.replace(\"9\",\"\")\n",
    "    text = text.lower()\n",
    "    \n",
    "    unique_chars = sorted(list(set(text)))\n",
    "    skip = 2 \n",
    "    char_len = 40\n",
    "    vocab_dict = {}\n",
    "    for i in range(len(unique_chars)):\n",
    "        vocab_dict[unique_chars[i]] = i\n",
    "    \n",
    "    #list of sets fo 40 characters\n",
    "    sequences = [] \n",
    "    # individual final characters \n",
    "    characters = []\n",
    "    \n",
    "    # generate seqs of 40 chars by looping through whole thing\n",
    "    for i in range(0, len(text)-41, skip):\n",
    "        sequences.append(text[i:i+40]) # sequence\n",
    "        characters.append(text[i+41]) # char \n",
    "    \n",
    "    # need to reshape because LSTM is being moody and wants a 3D thing for x\n",
    "    trainX = np.zeros((len(sequences), 40, len(unique_chars)))\n",
    "    trainY = np.zeros((len(sequences), len(unique_chars)))\n",
    "    # put 1s into the places where things fit the correct char\n",
    "    for index in range(len(sequences)):\n",
    "        for seq in range(len(sequences[0])):\n",
    "            trainX[index, seq] = get_char_repr(vocab_dict, sequences[index][seq])\n",
    "        trainY[index] = get_char_repr(vocab_dict, characters[index])\n",
    "        \n",
    "    return trainX,trainY, vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for line in fileinput.FileInput('data/shakespeare.txt'):\n",
    "    if line.rstrip():\n",
    "        text+=line.lstrip()\n",
    "        \n",
    "# do feature encoding and get vocab dictionary \n",
    "seqX, chars, vocab_map = preprocess_init(text)\n",
    "unique_chars = sorted(list(set(text)))\n",
    "map_len = len(vocab_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a character-based LSTM model\n",
    "\n",
    "def train_rnn(trainX, characters):\n",
    "    \n",
    "    model = Sequential()\n",
    "    # single layers of 100-200 LSTM units\n",
    "    model.add(LSTM(175, input_shape=(40,map_len)))\n",
    "    # fully connected dense output layer with a softmax nonlinearity\n",
    "    model.add(Dense(map_len, activation='softmax'))\n",
    "\n",
    "    # Train model to minimize categorical cross-entropy\n",
    "    model.compile(optimizer = 'rmsprop', # unclear if we need this\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics = [\"accuracy\"]    \n",
    "                  # we want accuracy over 0.6 on training data\n",
    "                 )\n",
    "\n",
    "    # train for many epochs so loss converges\n",
    "    print(\"Train model...\")\n",
    "    model.fit(trainX, characters, epochs = 30, batch_size = 64)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model...\n",
      "Epoch 1/30\n",
      "45739/45739 [==============================] - 44s 968us/step - loss: 2.8412 - accuracy: 0.1934\n",
      "Epoch 2/30\n",
      "45739/45739 [==============================] - 50s 1ms/step - loss: 2.6745 - accuracy: 0.2365\n",
      "Epoch 3/30\n",
      "45739/45739 [==============================] - 45s 987us/step - loss: 2.6021 - accuracy: 0.2534\n",
      "Epoch 4/30\n",
      "45739/45739 [==============================] - 50s 1ms/step - loss: 2.5462 - accuracy: 0.2648\n",
      "Epoch 5/30\n",
      "45739/45739 [==============================] - 44s 967us/step - loss: 2.4949 - accuracy: 0.2787\n",
      "Epoch 6/30\n",
      "45739/45739 [==============================] - 49s 1ms/step - loss: 2.4470 - accuracy: 0.2910\n",
      "Epoch 7/30\n",
      "45739/45739 [==============================] - 52s 1ms/step - loss: 2.3985 - accuracy: 0.3030\n",
      "Epoch 8/30\n",
      "45739/45739 [==============================] - 45s 987us/step - loss: 2.3529 - accuracy: 0.3122\n",
      "Epoch 9/30\n",
      "45739/45739 [==============================] - 45s 988us/step - loss: 2.3050 - accuracy: 0.3242\n",
      "Epoch 10/30\n",
      "45739/45739 [==============================] - 44s 970us/step - loss: 2.2580 - accuracy: 0.3355\n",
      "Epoch 11/30\n",
      "45739/45739 [==============================] - 43s 931us/step - loss: 2.2071 - accuracy: 0.3474\n",
      "Epoch 12/30\n",
      "45739/45739 [==============================] - 43s 948us/step - loss: 2.1544 - accuracy: 0.3643\n",
      "Epoch 13/30\n",
      "45739/45739 [==============================] - 45s 974us/step - loss: 2.0986 - accuracy: 0.3766\n",
      "Epoch 14/30\n",
      "45739/45739 [==============================] - 44s 953us/step - loss: 2.0402 - accuracy: 0.3925\n",
      "Epoch 15/30\n",
      "45739/45739 [==============================] - 46s 1ms/step - loss: 1.9815 - accuracy: 0.4082\n",
      "Epoch 16/30\n",
      "45739/45739 [==============================] - 43s 951us/step - loss: 1.9198 - accuracy: 0.4263\n",
      "Epoch 17/30\n",
      "45739/45739 [==============================] - 47s 1ms/step - loss: 1.8590 - accuracy: 0.4431\n",
      "Epoch 18/30\n",
      "45739/45739 [==============================] - 49s 1ms/step - loss: 1.7971 - accuracy: 0.4627\n",
      "Epoch 19/30\n",
      "45739/45739 [==============================] - 47s 1ms/step - loss: 1.7379 - accuracy: 0.4781\n",
      "Epoch 20/30\n",
      "45739/45739 [==============================] - 44s 971us/step - loss: 1.6792 - accuracy: 0.4964\n",
      "Epoch 21/30\n",
      "45739/45739 [==============================] - 47s 1ms/step - loss: 1.6215 - accuracy: 0.5159\n",
      "Epoch 22/30\n",
      "45739/45739 [==============================] - 48s 1ms/step - loss: 1.5639 - accuracy: 0.5325\n",
      "Epoch 23/30\n",
      "45739/45739 [==============================] - 48s 1ms/step - loss: 1.5113 - accuracy: 0.5478\n",
      "Epoch 24/30\n",
      "45739/45739 [==============================] - 55s 1ms/step - loss: 1.4626 - accuracy: 0.5619\n",
      "Epoch 25/30\n",
      "45739/45739 [==============================] - 42s 929us/step - loss: 1.4097 - accuracy: 0.5766\n",
      "Epoch 26/30\n",
      "45739/45739 [==============================] - 43s 933us/step - loss: 1.3631 - accuracy: 0.5937\n",
      "Epoch 27/30\n",
      "45739/45739 [==============================] - 45s 975us/step - loss: 1.3212 - accuracy: 0.6048\n",
      "Epoch 28/30\n",
      "45739/45739 [==============================] - 47s 1ms/step - loss: 1.2793 - accuracy: 0.6156\n",
      "Epoch 29/30\n",
      "45739/45739 [==============================] - 49s 1ms/step - loss: 1.2404 - accuracy: 0.6276\n",
      "Epoch 30/30\n",
      "45739/45739 [==============================] - 56s 1ms/step - loss: 1.2040 - accuracy: 0.6377\n"
     ]
    }
   ],
   "source": [
    "rnn_model = train_rnn(seqX, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_char(z_vector,temperature):\n",
    "    # implement temp parameter myself as part of the poem generation algo\n",
    "    # add a Lambda layer to LSTM during prediction or write a function\n",
    "    z_vector = np.asarray(z_vector)\n",
    "    num = np.exp(z_vector/temperature)\n",
    "    denom = np.sum(np.exp(z_vector/temperature))\n",
    "    index_predictions = num/denom\n",
    "    max_prediction_index = np.argmax(index_predictions)\n",
    "    \n",
    "    # have to find the char that goes with the index\n",
    "    char = \"\"\n",
    "    for i in vocab_map.keys():\n",
    "        if vocab_map[i] == max_prediction_index:\n",
    "            char = i\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Poetry Generation \n",
    "# to generate poems, draw softmax samples from the trained model\n",
    "def generate_sonnet_without_random_seed(temp):\n",
    "    generated_sonnet = ''\n",
    "    seed = \"shall i compare thee to a summer's day? \" # first sentence \n",
    "    generated_sonnet += seed\n",
    "    for line in range(13*40): # of there are 14 lines in a sonnet and our seed is the first\n",
    "        # each line is about 40 characters long?\n",
    "            \n",
    "        features = np.zeros((1,40,map_len))\n",
    "        for i in range(40):\n",
    "            features[0,i] = get_char_repr(vocab_map, generated_sonnet[len(generated_sonnet)-40+i])\n",
    "        \n",
    "        # generate the next value \n",
    "        predictions = rnn_model.predict(features)[0]\n",
    "        next_char = generate_next_char(predictions, temp) # change around the temp\n",
    "        generated_sonnet += next_char \n",
    "        \n",
    "    # separate poem by line and print out\n",
    "    for i in range(13):\n",
    "        if i==0:\n",
    "            print(generated_sonnet[i*40].upper()+generated_sonnet[i*40+1:(i+1)*40])\n",
    "        elif i>0 and i<12:\n",
    "            print(generated_sonnet[i*40].upper()+generated_sonnet[i*40+1:(i+1)*40]+\",\")\n",
    "        else:\n",
    "            print(generated_sonnet[i*40].upper()+generated_sonnet[i*41:(i+1)*40]+\".\")\n",
    "\n",
    "    return generated_sonnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Poem:\n",
      "====================\n",
      "Shall i compare thee to a summer's day? \n",
      "Hrwmod hte' eehrd,o a eo oh aeae  yulwti,\n",
      "N hn yu oferisd,t yutde ntin rmoe yu hsr,\n",
      "Snnt htrvne,hr iagtn o ees frmnwnnc,btog,\n",
      "Igtytosrwet  nr hm adahccccoe htrtnnes r,\n",
      "Cetrn.hrsnt n o aa hseas adoi o o a erns,\n",
      " ad,eti o hsrwran,ada o a eres o al eehr,\n",
      "D,o o hvr  o eu aa,hwsdflas htevr yu o o,\n",
      " a ee hsrsdae,o yu hreses h ol,adtntsrsi,\n",
      "G tetrmraotdt n ube o o al e,e o a aeweo,\n",
      " o yut hsres lef.tetrnm,adtntfaetym atig,\n",
      "T't' nterng o o a oehtba,adta adto hwetn,\n",
      "Oesasdadysadet nto adohae frm.\n"
     ]
    }
   ],
   "source": [
    "print('\\nSample Poem:\\n====================')\n",
    "t1 = generate_sonnet_without_random_seed(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_sonnet_without_random_seed() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e29a41b96f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_sonnet_without_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: generate_sonnet_without_random_seed() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "print('\\nSample Poem:\\n====================')\n",
    "t2 = generate_sonnet_without_random_seed(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nSample Poem:\\n====================')\n",
    "t3 = generate_sonnet_without_random_seed(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
