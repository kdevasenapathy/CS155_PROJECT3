{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Shakespearean Sonnets RNN Part 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import fileinput\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Processing\n",
    "- I'm not actually changing anything about the given text I'm literally just tokenizing the input and then passing the training data into the LSTM model.\n",
    "- I am making 5 char jumps between samples of 40\n",
    "- I tried to use the Keras tokenizer but it wasn't recognizing all of the unique character so I wrote my own\n",
    "- I also decided to remove new line characters because it ruined the format of the sonnet \n",
    "- I had a very high number of spaces, so I also removed white space from between the sonnets in the given text file and the numbers of the sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn text file into training set\n",
    "\n",
    "def get_char_repr(char_to_index, char):\n",
    "    unique_words = char_to_index.keys()\n",
    "    # Return a vector that's zero everywhere besides the index corresponding to <word>\n",
    "    feature_representation = np.zeros(len(unique_words))\n",
    "    feature_representation[char_to_index[char]] = 1\n",
    "    return feature_representation \n",
    "\n",
    "def preprocess_init(text):\n",
    "    # Convert text to dataset using semi-redundant sequences\n",
    "    text = text.replace(\"\\n\",\"\") # maybe add this back? idk \n",
    "    text = text.replace(\"0\",\"\")\n",
    "    text = text.replace(\"1\",\"\")\n",
    "    text = text.replace(\"2\",\"\")\n",
    "    text = text.replace(\"3\",\"\")\n",
    "    text = text.replace(\"4\",\"\")\n",
    "    text = text.replace(\"5\",\"\")\n",
    "    text = text.replace(\"6\",\"\")\n",
    "    text = text.replace(\"7\",\"\")\n",
    "    text = text.replace(\"8\",\"\")\n",
    "    text = text.replace(\"9\",\"\")\n",
    "    text = text.lower()\n",
    "    \n",
    "    unique_chars = sorted(list(set(text)))\n",
    "    skip = 2 \n",
    "    seq_len = 40\n",
    "    vocab_dict = {}\n",
    "    for i in range(len(unique_chars)):\n",
    "        vocab_dict[unique_chars[i]] = i\n",
    "    \n",
    "    #list of sets fo 40 characters\n",
    "    sequences = [] \n",
    "    # individual final characters \n",
    "    characters = []\n",
    "    \n",
    "    # generate seqs of 40 chars by looping through whole thing\n",
    "    # but semi redundant \n",
    "    for i in range(0, len(text)-seq_len, skip):\n",
    "        sequences.append(text[i:i+seq_len]) # sequence\n",
    "        characters.append(text[i+seq_len]) # char \n",
    "    \n",
    "    # need to reshape because LSTM is being moody and wants a 3D thing for x\n",
    "    trainX = np.zeros((len(sequences), seq_len, len(unique_chars)))\n",
    "    trainY = np.zeros((len(sequences), len(unique_chars)))\n",
    "    # put 1s into the places where things fit the correct char\n",
    "    for index in range(len(sequences)):\n",
    "        for seq in range(len(sequences[0])):\n",
    "            trainX[index, seq] = get_char_repr(vocab_dict, sequences[index][seq])\n",
    "        trainY[index] = get_char_repr(vocab_dict, characters[index])\n",
    "        \n",
    "    return trainX,trainY, vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for line in fileinput.FileInput('data/shakespeare.txt'):\n",
    "    if line.rstrip():\n",
    "        text+=line.lstrip()\n",
    "        \n",
    "# do feature encoding and get vocab dictionary \n",
    "seqX, chars, vocab_map = preprocess_init(text)\n",
    "unique_chars = sorted(list(set(text)))\n",
    "map_len = len(vocab_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a character-based LSTM model\n",
    "\n",
    "def train_rnn(trainX, characters):\n",
    "    \n",
    "    model = Sequential()\n",
    "    # single layers of 100-200 LSTM units\n",
    "    model.add(LSTM(175, input_shape=(40,map_len)))\n",
    "    # fully connected dense output layer with a softmax nonlinearity\n",
    "    model.add(Dense(map_len, activation='softmax'))\n",
    "\n",
    "    # Train model to minimize categorical cross-entropy\n",
    "    model.compile(optimizer = 'rmsprop', # unclear if we need this\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics = [\"accuracy\"]    \n",
    "                  # we want accuracy over 0.6 on training data\n",
    "                 )\n",
    "\n",
    "    # train for many epochs so loss converges\n",
    "    print(\"Train model...\")\n",
    "    model.fit(trainX, characters, epochs = 12, batch_size = 32)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model...\n",
      "Epoch 1/12\n",
      "45740/45740 [==============================] - 63s 1ms/step - loss: 2.4915 - accuracy: 0.2895\n",
      "Epoch 2/12\n",
      "45740/45740 [==============================] - 75s 2ms/step - loss: 2.1387 - accuracy: 0.3755\n",
      "Epoch 3/12\n",
      "45740/45740 [==============================] - 75s 2ms/step - loss: 1.9912 - accuracy: 0.4140\n",
      "Epoch 4/12\n",
      "13920/45740 [========>.....................] - ETA: 1:04 - loss: 1.8926 - accuracy: 0.4369"
     ]
    }
   ],
   "source": [
    "rnn_model = train_rnn(seqX, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_char(z_vector,temperature):\n",
    "    # implement temp parameter myself as part of the poem generation algo\n",
    "    # add a Lambda layer to LSTM during prediction or write a function\n",
    "    z_vector = np.asarray(z_vector)\n",
    "    num = np.exp(z_vector/temperature)\n",
    "    denom = np.sum(np.exp(z_vector/temperature))\n",
    "    index_predictions = num/denom\n",
    "    max_prediction_index = np.argmax(index_predictions)\n",
    "    \n",
    "    # have to find the char that goes with the index\n",
    "    char = \"\"\n",
    "    for i in vocab_map.keys():\n",
    "        if vocab_map[i] == max_prediction_index:\n",
    "            char = i\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Poetry Generation \n",
    "# to generate poems, draw softmax samples from the trained model\n",
    "def generate_sonnet_without_random_seed(temp):\n",
    "    generated_sonnet = ''\n",
    "    seed = \"shall i compare thee to a summer's day? \" # first sentence \n",
    "    generated_sonnet += seed\n",
    "    for line in range(13*40): # of there are 14 lines in a sonnet and our seed is the first\n",
    "        # each line is about 40 characters long?\n",
    "            \n",
    "        features = np.zeros((1,40,map_len))\n",
    "        for i in range(40):\n",
    "            features[0,i] = get_char_repr(vocab_map, generated_sonnet[len(generated_sonnet)-40+i])\n",
    "        \n",
    "        # generate the next value \n",
    "        #print(features.shape)\n",
    "        predictions = rnn_model.predict(features)[0]\n",
    "        next_char = generate_next_char(predictions, temp) # change around the temp\n",
    "        generated_sonnet += next_char \n",
    "        \n",
    "    # separate poem by line and print out\n",
    "    for i in range(13):\n",
    "        if i==0:\n",
    "            print(generated_sonnet[i*40].upper()+generated_sonnet[i*40+1:(i+1)*40])\n",
    "        elif i>0 and i<12:\n",
    "            print(generated_sonnet[i*40].upper()+generated_sonnet[i*40+1:(i+1)*40]+\",\")\n",
    "        else:\n",
    "            print(generated_sonnet[i*40].upper()+generated_sonnet[i*41:(i+1)*40]+\".\")\n",
    "\n",
    "    return generated_sonnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nSample Poem:\\n====================')\n",
    "t1 = generate_sonnet_without_random_seed(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nSample Poem:\\n====================')\n",
    "t2 = generate_sonnet_without_random_seed(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nSample Poem:\\n====================')\n",
    "t3 = generate_sonnet_without_random_seed(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
